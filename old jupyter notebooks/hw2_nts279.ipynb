{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 373 Machine Learning Toolbox for Text Analysis, Spring 2019\n",
    "\n",
    "# Homework 2 - due Tuesday Feb 25 2020 at 2:00pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw2_YourEID.ipynb``\n",
    "- a PDF version named `hw2_YourEID.pdf`\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer. In addition, you will upload a PDF version of this notebook with all outputs included (`File` >> `Download As` >> `PDF via LaTeX`). If you do not have latex on your machine, you should do this by using print preview and then printing as PDF (`File` >> `Print Preview`)\n",
    "\n",
    "A perfect solution for this homework is worth **100** points. **There is also a 10 point bonus assignment.** For non-programming tasks, you must show work to get partial credit. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a thread on Piazza where you can look for a study group: https://piazza.com/class/k5o768x0bq6ai?cid=10\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jjessyli.github.io/courses/lin373#extension-policy\n",
    "\n",
    "For typing up your answers to problems 1 and 2, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Naive Bayes\n",
    "\n",
    "We'll do a bit of manual parameter estimation here.\n",
    "\n",
    "## **(a)** (20 points) \n",
    "Calculate the sufficient parameters for Naive Bayes using the data in the figure below, that\n",
    "is, the prior class probabilities and the conditional probabilities for all of the symbols.\n",
    "\n",
    "X | Y\n",
    "--| --\n",
    "a b b b c b | 1\n",
    "c a c c c b | -1\n",
    "c c c b | -1\n",
    "b a b b b | 1\n",
    "b c a b b | ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 1:\n",
    "abbbcb : (a: 1, b: 4, c: 1)\n",
    "babbb  : (a: 1, b: 4, c: 0)\n",
    "totals : (a: 2, b: 8, c: 1)\n",
    "size   : 11\n",
    "\n",
    "Group -1:\n",
    "cacccb : (a: 1, b: 1, c: 4)\n",
    "cccb   : (a: 0, b: 1, c: 3)\n",
    "totals : (a: 1, b: 2, c: 7)\n",
    "size   :  10\n",
    "\n",
    "priors\n",
    "p(1) = 1/2\n",
    "p(-1) = 1/2\n",
    "\n",
    "conditionals\n",
    "p(a|1) = 2/11\n",
    "p(b|1) = 8/11\n",
    "p(c|1) = 1/11\n",
    "\n",
    "p(a|-1) = 1/10\n",
    "p(b|-1) = 2/10\n",
    "p(c|-1) = 7/10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(b)** (5 points) \n",
    "Using these, calculate the most likely class (1 or -1) for the unlabeled example (currently\n",
    "labeled `???`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group1\n",
    "p(1)*P(b|1)^3*P(c|1)*P(a|1) = (1/2) * (8/11)^3* (2/11) * (1/11) = 0.00317911717\n",
    "\n",
    "\n",
    "\n",
    "Group2\n",
    "P(-1)*P(b|-1)^3*P(a|-1)*P(c|-1) = (1/2) * (2/10)^2 *(1/10)*(7/10) = 0.0014\n",
    "\n",
    "\n",
    "The unknown is predicted to be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Language Modeling\n",
    "\n",
    "In this problem, we will implement our own Shakespeare and Austen language models!\n",
    "\n",
    "### Data\n",
    "\n",
    "We will be working with a few different files. \n",
    "- `shakespeare.txt`\n",
    "\n",
    "- `shakespeare_short.txt`\n",
    "- `austen.txt`\n",
    "- `austen_short.txt`\n",
    "- `test.txt`\n",
    "\n",
    "You can use the \"short\" text files to do testing and debugging as the other files will take a little while to run! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Clean text. (5 points) \n",
    "\n",
    "Create a function `sent_transform(sent_string)`, such that when given a sentence as a string, it would return a `list` of words, lower-cased. Use NLTK's `word_tokenize` function to tokenize the sentence. For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> sent_transform(\"ROSALIND. Why, whither shall we go?\")\n",
    ">>> ['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def sent_transform(sentence_string):\n",
    "    words = nltk.word_tokenize(sentence_string.lower())\n",
    "    return words\n",
    "\n",
    "print(sent_transform(\"ROSALIND. Why, whither shall we go?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Find n-grams. (10 points) \n",
    "\n",
    "Create a function `make_ngram_tuples(words, n)` that returns a sequence of all n-grams seen in the input, in order.  The function returns a sequence of tuples where each tuple is of the form `(context, word)`.  The context is a tuple of the preceding n−1 words for each word; if the number of preceding words is smaller than n−1, place a `<s>` symbol in place of each missing context.  Close each sentence with an additional token `</s>`.  You can assumen n>1, that is, we are interested in enumerating bigrams, trigrams etc, and not unigrams.\n",
    "\n",
    "For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> samples = [’she’, ’eats’, ’happily’]\n",
    ">>> make_ngram_tuples(samples, 2)\n",
    "[((’<s>’,), ’she’), ((’she’,), ’eats’), ((’eats’,), ’happily’), ((’happily’,), ’</s>’)]\n",
    ">>> make_ngram_tuples(samples, 3)\n",
    "\n",
    "[((’<s>’, ’<s>’), ’she’), ((’<s>’, ’she’), ’eats’), ((’she’, ’eats’), ’happily’),((’eats’, ’happily’), ’</s>’)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<s>', '<s>'), 'she'), (('<s>', 'she'), 'eats'), (('she', 'eats'), 'happily'), (('eats', 'happily'), '<s>')]\n"
     ]
    }
   ],
   "source": [
    "def make_ngram_tuples(samples,n):\n",
    "    ngram_tuple = []\n",
    "    for ind in range(len(samples)):\n",
    "        ngram_tuple.append((helper_fun(ind,n,samples)\n",
    "                           ,samples[ind]))\n",
    "    ngram_tuple.append((helper_fun(len(samples),n, samples),'<s>'))\n",
    "    return ngram_tuple\n",
    "        \n",
    "        \n",
    "def helper_fun(ind, n,samples):\n",
    "    tuple_context = []\n",
    "    backwards = ind - n + 1\n",
    "    while backwards != ind:\n",
    "        if backwards < 0:\n",
    "            tuple_context.append('<s>')\n",
    "        else:\n",
    "            tuple_context.append(samples[backwards])\n",
    "        backwards +=1\n",
    "    return tuple(tuple_context)\n",
    "        \n",
    "    \n",
    "    \n",
    "print(make_ngram_tuples( ['she', 'eats', 'happily'],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  Building a bigram language model (30 points)\n",
    "\n",
    " We are now ready to estimate a bigram language model from the training data.  We will use **Laplace smoothing (add-1)**. With our model on a laptop with 16g RAM, it took <20 seconds to run the Shakespeare model.\n",
    "\n",
    "Define a class `BigramModel` with the following instance methods:\n",
    "\n",
    "* `__init__(self, inputfile)`: the  constructor  that  builds  a  bigram  language  model  from  an input file.  Assume  file paths. The language model ought to be able to handle words not seen in the training data.  Such words will most certainly appear if the LM is used in some application to estimate the likelihood of new data.   There  are  many  ways  to  incorporate  unknown  vocabulary  in  a  language  model.   In  this problem, we will take all words that appear only once and replace them with the symbol `<UNK>`. \n",
    "\n",
    "* `logprob(self, context, word)`:  returns the log probability of the current word given its context. Note that you need to pre-process the input: words should be lower-cased and out of vocabulary words should be replaced by `<UNK>`s.\n",
    "\n",
    "* `get_ppl(self, testfile)`: given a testfile file, returns the perplexity of the model computed for that particular text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "\n",
    "class BigramModel(object):\n",
    "    \n",
    "    def __init__(self, inputfile, alpha):\n",
    "        \n",
    "        # initials\n",
    "        self.alpha = alpha\n",
    "        self.clean_input = self.clean_input_func(inputfile)\n",
    "        self.word_counts = Counter(self.clean_input)\n",
    "        self.unknown_words_dict = self.getUnknownWords(self.word_counts)\n",
    "        self.vocab = set(self.unknown_words_dict.keys())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        \n",
    "        # bigram part\n",
    "        self.bigrams = nltk.bigrams(self.clean_input)\n",
    "        self.bigram_dict = self.get_bigram_dictionary(self.bigrams)\n",
    "        \n",
    "        \n",
    "    #############################################################################################################33\n",
    "    # bigram functions\n",
    "    #\n",
    "    #\n",
    "    #################################################################################################################\n",
    "    def get_bigram_freq(self, word1,word2):\n",
    "        return np.log2(self.bigram_dict[word1][word2] + self.alpha)-np.log2(self.word_counts[word2] + (self.alpha * self.vocab_size))\n",
    "    \n",
    "        # I am assuming that the context is given as a string and the word is given as a string\n",
    "    def logprob(self, context, word): \n",
    "        sent = nltk.bigrams(nltk.word_tokenize(context.lower()) + [word])\n",
    "        return np.sum([(self.get_bigram_freq(self.get_valid_word(word1), self.get_valid_word(word2))\n",
    "                       ) for word1, word2 in sent])\n",
    "    \n",
    "    def logprob_sentences(self, sent): \n",
    "        return np.sum([(self.get_bigram_freq(self.get_valid_word(word1), self.get_valid_word(word2))\n",
    "                       ) for word1, word2 in sent])\n",
    "    \n",
    "    def get_ppl(self, testfile):\n",
    "        sentences = self.clean_input_func(testfile)\n",
    "        sent = [bi for bi in nltk.bigrams(sentences)]\n",
    "        log_sent_freq = self.logprob_sentences(sent)\n",
    "        return 2 ** (-1 * log_sent_freq/len(sent))\n",
    "    \n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    ################################################################################################################\n",
    "    # preprocessing and helper functions for txt files\n",
    "    #\n",
    "    #\n",
    "    ##################################################################################################################\n",
    "\n",
    "\n",
    "    # function gets amount of words that have a count of 1\n",
    "    # Makes new dictionary that has all words with more than one count and the count of all unknown words\n",
    "    def getUnknownWords(self,word_dict):\n",
    "        unknown_words = 0\n",
    "        dictionary_with_unknown_words = {}\n",
    "        for k,v in word_dict.items():\n",
    "            if v == 1:\n",
    "                unknown_words += 1\n",
    "            else:\n",
    "                dictionary_with_unknown_words[k] = v\n",
    "        dictionary_with_unknown_words[\"^unk^\"] = unknown_words\n",
    "        return dictionary_with_unknown_words\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # This function creates a bigram dictionary. It gets counts for all bigrams\n",
    "    def get_bigram_dictionary(self,bigrams):\n",
    "        d_bigram = defaultdict(Counter)\n",
    "        for i in bigrams:\n",
    "            d_bigram[self.get_valid_word(i[0])][self.get_valid_word(i[1])] +=1\n",
    "        return d_bigram\n",
    "    \n",
    "    #\n",
    "    # This function checks if a word is in the vocab and returns the word or unk if not in vocab\n",
    "    def get_valid_word(self,word):\n",
    "        if word.lower() in self.vocab:\n",
    "            return word.lower()\n",
    "        else:\n",
    "            return \"^unk^\"\n",
    "        \n",
    "    #\n",
    "    # This function will open the text file,\n",
    "    # separate sentences out using nltk\n",
    "    # at an ending marking and starting marker\n",
    "    # Then concat a word tokenized list of sentences together\n",
    "    # we get a bag of words where sentences are distinguised by the markers\n",
    "    def clean_input_func(self,input_file):\n",
    "        text = []\n",
    "        with open(input_file, encoding=\"utf8\") as file:\n",
    "             sentences = nltk.sent_tokenize(file.read())\n",
    "        for sent in sentences:\n",
    "            sent = sent.lower()\n",
    "            whole_sentence = ['<s>'] + nltk.word_tokenize(sent) + ['</s>']\n",
    "            text = text + whole_sentence\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bigram_Shake = BigramModel(\"data/shakespeare.txt\",.01)\n",
    "print(my_bigram_Shake.bigram_dict['the'].most_common(5)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.46666854333515\n",
      "37.06193930567681\n"
     ]
    }
   ],
   "source": [
    "my_bigram_Shake = BigramModel(\"data/shakespeare.txt\",.01)\n",
    "my_bigram_Austen = BigramModel(\"data/austen.txt\",.01)\n",
    "print(my_bigram_Austen.get_ppl(\"data/test.txt\"))\n",
    "print(my_bigram_Shake.get_ppl(\"data/test.txt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Random text generator\n",
    "\n",
    "(15 points) Now, we are ready to generate some Shakespeare text! Starting with the sentence start symbol `<s>`, at each step, use the previously generated word as context, and sample one of the top 5 words that follow this word. We stop whenever we hit `</s>`, or when we have generated a 20-word sentence, whichever is earlier.\n",
    "\n",
    "Implement a function `text_generator(bigramlm, n)` that takes a BigramModel---trained on our Shakespeare text---as input, and generates `n` random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> she had not be a very good humour and the house , and i am glad of her own room\n",
      "<s> it , and i have the world to have not know what he could have a few days together ;\n",
      "<s> `` oh no one . ' </s>\n",
      "<s> it was in their acquaintance . ' '' was the world to the same . </s>\n",
      "<s> `` yes ; `` you , and she felt . </s>\n",
      "<s> it , she could do . '' cried mary , and i have no , '' said fanny . </s>\n",
      "<s> it would have the house in a great deal to her to see her to the same moment ; i\n",
      "<s> she had a great deal to her . ' said he was the same time , `` but it was\n",
      "<s> it , `` but i am very well to see you are , she could not have been so very\n",
      "<s> she would have the world ! ' '' cried , i am glad to her . ' '' said ,\n",
      "<s> the same , and the first , '' cried mrs. allen was not know that the same . '' was\n",
      "<s> i am sure , '' was a great , '' cried catherine 's being in their father had not to\n",
      "<s> she felt . </s>\n",
      "<s> she would be the house , and , she would have no doubt , '' was to her ; but\n",
      "<s> it was not have no longer , '' was a very much to the first . ' </s>\n",
      "<s> she could have not to have the first to her . ] </s>\n",
      "<s> she would rather have a great house ; and the first to her , she had no doubt that i\n",
      "<s> `` you have been a very well , `` but i have the world , she would not have no\n",
      "<s> she felt the first time to the first . ) </s>\n",
      "<s> `` yes , and she would rather the house , and , `` yes . </s>\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def text_generator(bigramLm, n):\n",
    "    for i in range(n):\n",
    "        start = ['<s>']\n",
    "        end = '</s>'\n",
    "        next_word = start\n",
    "        while next_word[-1] != end and len(next_word) < 21:\n",
    "            top_5 = bigramLm.bigram_dict[next_word[-1]].most_common(5)\n",
    "            rand_range_fix = random_fix(top_5)\n",
    "            random_num = random.randrange(0,rand_range_fix,1)\n",
    "            future_word = top_5[random_num][0]\n",
    "            if future_word == \"^unk^\":\n",
    "                continue\n",
    "            next_word += [future_word]\n",
    "        \n",
    "        # put words into sentence and print\n",
    "        print(\" \".join(next_word))\n",
    "    return \"done\"\n",
    "    \n",
    "# sometimes there isn't a top 5 because there are too few words\n",
    "# so we need this function\n",
    "def random_fix(top5):\n",
    "    top_5_length = len(top5)\n",
    "    if top_5_length < 5:\n",
    "        return top_5_length\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(text_generator(my_bigram_Austen, 20))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Authorship attribution (15 points)\n",
    "\n",
    "If we have built language models of multiple authors, they can be used to check the author of an unknown piece of writing. Concretely, for the unknown text, we can run the file through each known author's language model, and use perplexity to predict which author the unknown text is most likely to belong to.\n",
    "\n",
    "In this problem, we build four language models:\n",
    "- a  Shakespeare language model\n",
    "- a  Jane Austen language model\n",
    "\n",
    "Once you have trained both models, use the `get_ppl` function from each language model on the file `test.txt`. \n",
    "\n",
    "Who wrote the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.46666854333515\n",
      "37.06193930567681\n"
     ]
    }
   ],
   "source": [
    "my_bigram_Shake = BigramModel(\"data/shakespeare.txt\",.01)\n",
    "my_bigram_Austen = BigramModel(\"data/austen.txt\",.01)\n",
    "print(my_bigram_Austen.get_ppl(\"data/test.txt\"))\n",
    "print(my_bigram_Shake.get_ppl(\"data/test.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jane Austen wrote the test script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (10 points): Data Annotation\n",
    "\n",
    "This bonus problem is about a first-hand experience of **gathering** data. We will first annotate data for an existing research project, and later in the semester, we will analyze this dataset that y'all annotated. We hope you find the data fun to look at!\n",
    "\n",
    "If you choose to do this problem, we also really, really appreciate your help in this research project that's ongoing in the Linguistics department. The annotations will *not* be anonymous because of grading reasons (we need to know that you've done the task to award you points), but *no* personal information will be collected.\n",
    "\n",
    "We will be using a popular crowdsourcing platform: the Amazon Mechanical Turk. Here, we will look at it from the *annotator*'s angle; in class later in the semester, we will also look at it from a *data collector*'s angle. Since this is a small class experiment, we will only work with their sandbox version, i.e., unfortunately, no real money involved.\n",
    "\n",
    "* 1) In order to annotate data, you need to setup yourselves as an Amazon Mechanical Turk (AMT) \"worker\"\n",
    "  * a) go to this url and log in with your amazon.com credentials (use the same user/password when you shop on Amazon): https://workersandbox.mturk.com/\n",
    "  * b) complete the steps to register as an AMT worker\n",
    "  \n",
    "* 2) Find the AMT task:\n",
    "  * a) after registering to be a worker (see above), go to this url: https://workersandbox.mturk.com/projects/3QZMXSMFJ7GIP9G367LB3IAC25JKL2/tasks\n",
    "  * b) you should see a page like the one below. Confirm the requester is \"CompDisc” and the title begins with “What is the intent...”\n",
    "  <img src=\"images/1.png\" alt=\"bonus question\" width=\"600\"/>\n",
    "  \n",
    "* 3) Complete the AMT task:\n",
    "  * a) Click on the orange “Accept\" button in the upper-right corner\n",
    "  <img src=\"images/2.png\" alt=\"bonus question\" width=\"600\"/>\n",
    "  * b) Read and follow the instructions. After answering all the questions, click on the “Submit” button at the bottom-left of the page.\n",
    "  <img src=\"images/3.png\" alt=\"bonus question\" width=\"600\"/>\n",
    "  \n",
    "* c) **(Optional)** If you find this task fun, repeat the above steps to complete more HITs (you can complete up to 6 HITs). The instructions are the same so you can skip them by clicking on “Click to hide/show”.\n",
    "  <img src=\"images/4.png\" alt=\"bonus question\" width=\"600\"/>\n",
    "  \n",
    "* d) Copy your worker id in the upper left corner of the page, and paste it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR6AHNQNG8HN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we would like to bring this experiment to the public, please share any feedback, good or bad! For example, let us know if you felt there were any unclear or ambiguous parts, or anything that could be clarified or made more intuitive to relieve the need for lengthy instructions.\n",
    "\n",
    "Enter any comments below:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
