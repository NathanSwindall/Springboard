{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 373 Machine Learning Toolbox for Text Analysis, Spring 2019\n",
    "\n",
    "# Homework 1 - due Tuesday Feb 11 2020 at 2:00pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "- a PDF version named `hw1_YourEID.pdf`\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer. In addition, you will upload a PDF version of this notebook with all outputs included (`File` >> `Download As` >> `PDF via LaTeX`). If you do not have latex on your machine, you should do this by using print preview and then printing as PDF (`File` >> `Print Preview`)\n",
    "\n",
    "A perfect solution for this homework is worth 95 points but will be counted out of 100. If you completed homework 0, you will automatically receive an additional 5 points.  For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a thread on Piazza where you can look for a study group: https://piazza.com/class/k5o768x0bq6ai?cid=10\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jjessyli.github.io/courses/lin373#extension-policy\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- Marcos\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: the paradox of induction (15 points)\n",
    "\n",
    "Consider a statement whose truth is unknown. If we see many examples that are compatible with it, we are tempted to view the statement as more probable. Such reasoning is often referred to as _inductive inference_ (in a philosophical, rather than mathematical sense). Consider now the statement that \"all cows are white\". An equivalent statement is that \"everything that is not white is not a cow\". We then observe several black panthers. Our observations are clearly compatible with the statement, but do they make the hypothesis \"all cows are white\" more likely?\n",
    "\n",
    "To analyze such a situation, we consider a probabilistic model. Let us assume that there are two possible states of the world, which we model as complementary events:\n",
    "\n",
    "<center>$A$: all cows are white,\n",
    "    \n",
    "<center>$A^c$: 50% of all cows are white.\n",
    "\n",
    "Let $p$ be the prior probability $P(A)$ that all cows are white. We make an observation of a cow or a panther, with probability $q$ and $1-q$, respectively, independent of whether event $A$ occurs or not. Assume that $0<p<1, 0<q<1$, and that all panthers are black.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Given the event $B=$\\{a black panther was observed\\}, what is $P(A|B)$? Show your work (5pts)\n",
    "\n",
    "Since A is independent of B, then the probability of P(A|B) is just P(A) = p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Given the event $C=$\\{a white cow was observed\\}, what was $P(A|C)$? Show your work (5pts)\n",
    "\n",
    "We have the equation P(A|C) = P(C|A) P(A) / P(C). We know that P(A) = p. The for P(C|A), we know that if a cow is white that if we observe a cow then there will be a probability of 1 * q. P(C) is the probability of the white cow being observed so we take the whole sample space into account. Thus, we have q * p + 1\\2 * q * (1-p). Therefore, our equation is 1* q*p / (q*p + .5 * q*(1-p), which simplifies to p/(p + .5 * (1-P))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Which is larger? Explain the implication. (5pts)\n",
    "\n",
    "Since we have one that is p and another one where p is being divided, then P(A|B) is the bigger answer. This implies that if something has nothing to do with another thing, then one probability will not affect the other as in the case of having observed a black panther. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Problem 2: MAP (10 pts)\n",
    "\n",
    "We have discussed the Bernoulli distribution. Now, if you flip the coin $N$ times, with $H$ heads and $T$ tails, the likelihood of observing a sequence (data) $D$ is:\n",
    "$$\n",
    "P(D|\\theta) = \\theta^H(1-\\theta)^T\n",
    "$$\n",
    "\n",
    "In the Bayesian framework, we assume that we have some prior knowledge of $\\theta$ which can be described with a distribution $P(\\theta)$. \n",
    "The Beta distribution $Beta(\\alpha;\\beta)$ is often used as this prior, which, combined with our observed data $D$, leads to the posterior distribution $P(\\theta|D)$. \n",
    "\n",
    "But what if we want a single number (an estimate) of our \"best\" $\\theta$? This value will no longer be the same as the maximum likelihood estimate $\\hat{\\theta}_{MLE}$. Instead, this new \"best\" parameter, dubbed $\\hat{\\theta}_{MAP}$, is called _maximum a posterior_ or MAP: $$\\hat{\\theta}_{MAP}=\\text{argmax}_\\theta(P(\\theta|D)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is $\\hat{\\theta}_{MAP}$? Express it in terms of $H,T,\\alpha,\\beta$.\n",
    "This is just the same as solving of MLE. Though we start off with the equation $\\theta^{ \\alpha - 1 + H}(1-\\theta)^{\\beta -1 + T}$. The maximum a posterior will give us: $\\frac{H + (\\alpha - 1)}{H+(\\alpha -1) + (\\beta -1) + T}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Decision Trees (30 points)\n",
    "\n",
    "Consider the following set of training examples where we have two features, $X_1$ and $X_2$, and the goal is to predict the target $Y$. Each row indicates the values observed, and how many times that set of values was observed. For example, $(+,T,T)$ was observed 3 times, while $(−,T,T)$ was never observed.\n",
    "\n",
    "|Y | $X_1$ | $X_2$ | Count|\n",
    "|-|-|-|-|\n",
    "|+ | T | T | 3|\n",
    "|+ | T | F | 4|\n",
    "|+ | F | T | 4|\n",
    "|+ | F | F | 1|\n",
    "|- | T | T | 0|\n",
    "|- | T | F | 1|\n",
    "|- | F | T | 3|\n",
    "|- | F | F | 5 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is the sample entropy $H(Y)$ for this training data (with logarithms base 2), _before_ we start splitting on either feature? (10 pts)\n",
    "\n",
    "The entropy $H(Y)$ before splitting the training data is $H(12,9) = -\\frac{12}{21} log_2 \\frac{12}{21} -\\frac{9}{21} log_2 \\frac{9}{21}$ which is equal to 0.985\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)  Should we first split on $X_1$ or $X_2$? Calculate the information gains for each feature. (20 pts)\n",
    "\n",
    "First we need to get the numbers for $X_1$. For $X_1T$ we get the tuple (7.0,1.0) which will give us the function H(7,1) = .54356. Then, we need to get the $X_1F$ part which is the tuple (5,8). This will give us the function H(5,8) = -0.96123. Then when we want to get the combined entropy we get 0.8021. Finally, the gain for this example is 0.1831. \n",
    "\n",
    "Now for the next part. For $X_2T$ we have the tuple (7,3) for the function H(7,3) which gives us 0.8812. Then we have the tuple (5,6) which gives us H(5,6) = 0.9940. The combined entropy is 0.940344. Finally the gain for $X_2$ is 0.04488. Thus, it would be better to split on $X_1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4:  log odds ratios (40 points)\n",
    "This exercise is an exploratory analysis of the Sentiment140 dataset. Sentiment140 combines 160K tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) and neutral (2). You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "In this problem, we will analyze how often a word tend to appear with a positive sentiment vs. a negative one. The metric we are going to use is  **log odds ratio**, that compares the conditional probability of a word occurring in one type of sentences, say, positive ($P(word|pos)$), and the word occurring in another type of sentences, say, negative ($P(word|neg)$):\n",
    "$$log\\_odds\\_ratio(word, pos) = \\log\\frac{P(word|pos)}{P(word|neg)}$$\n",
    "The higher the $log\\_odds\\_ratio$, the more likely the word is associated with positive sentences.\n",
    "\n",
    "We will use the Sentiment140 dataset. Sentiment140 combines 1.6 million tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) or neutral (2). _We will  **not** consider neutral tweets in this problem_. You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "Download from Canvas the file ``sentiment140_sample1.csv`` ---a 10K sample from the training set of Sentiment140---and put it under the  **same directory** (folder) as your python script or notebook file. As a reminder, the file is formatted under six fields, including polarity, tweet ID, date, query username and the text of the tweet. We will only use polarity and tweet text in this assignment.\n",
    "\n",
    "In the following exercises, we have provided several expected inputs and outputs of the functions that you will implement. Treat these as test cases for your code; if you get numbers very far off from what is listed here with the same input, you have bugs to crush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467817374</td>\n",
       "      <td>Mon Apr 06 22:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ajaxpro</td>\n",
       "      <td>@MissXu sorry! bed time came here (GMT+1)   ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467863716</td>\n",
       "      <td>Mon Apr 06 22:33:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stacyc37</td>\n",
       "      <td>sad that the 'feet' of my macbook just fell off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467878057</td>\n",
       "      <td>Mon Apr 06 22:37:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>debbieseraphina</td>\n",
       "      <td>help me forget 8th april &amp;amp; 13th july!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467909292</td>\n",
       "      <td>Mon Apr 06 22:45:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>satori</td>\n",
       "      <td>@soillodge yes, it will be. it's only Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1468045043</td>\n",
       "      <td>Mon Apr 06 23:25:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TigerHasse</td>\n",
       "      <td>Debbugging old VB6 code, the day could have st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467817374  Mon Apr 06 22:21:30 PDT 2009  NO_QUERY          ajaxpro   \n",
       "1  0  1467863716  Mon Apr 06 22:33:35 PDT 2009  NO_QUERY         stacyc37   \n",
       "2  0  1467878057  Mon Apr 06 22:37:26 PDT 2009  NO_QUERY  debbieseraphina   \n",
       "3  0  1467909292  Mon Apr 06 22:45:54 PDT 2009  NO_QUERY           satori   \n",
       "4  0  1468045043  Mon Apr 06 23:25:27 PDT 2009  NO_QUERY       TigerHasse   \n",
       "\n",
       "                                                   5  \n",
       "0  @MissXu sorry! bed time came here (GMT+1)   ht...  \n",
       "1   sad that the 'feet' of my macbook just fell off   \n",
       "2         help me forget 8th april &amp; 13th july!   \n",
       "3      @soillodge yes, it will be. it's only Monday   \n",
       "4  Debbugging old VB6 code, the day could have st...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "import pandas\n",
    "sentiment_data = pandas.read_csv(\"sentiment140_sample1.csv\", header = None, encoding = \"ISO-8859-1\")\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2.009200</td>\n",
       "      <td>1.997586e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.000079</td>\n",
       "      <td>1.951833e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467817e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.956556e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.002018e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.176924e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.329177e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  10000.000000  1.000000e+04\n",
       "mean       2.009200  1.997586e+09\n",
       "std        2.000079  1.951833e+08\n",
       "min        0.000000  1.467817e+09\n",
       "25%        0.000000  1.956556e+09\n",
       "50%        4.000000  2.002018e+09\n",
       "75%        4.000000  2.176924e+09\n",
       "max        4.000000  2.329177e+09"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Frequency counts  (10 points)\n",
    "First, let's create dictionaries that record the count of each word in positive tweets, as well as the count of each word in negative tweets. Here, here, ``counts[\"pos\"]`` will contain key-value pairs of a word and its number of appearance in positive tweets, ``counts[\"neg\"]`` will contain key-value pairs of a word and its number of appearance in negative tweets\n",
    "\n",
    "To parse the tweets, we will use NLTK's ``word_tokenize()`` function. As an example, the following tokenizes a sentence into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #you only have to do this once per environment\n",
    "\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-casing all words gives cleaner counts. For example, consider the two sentences: \"Apples are delicious. John loves apples.\" If we do not lower-case each word, ''Apples'' and ''apples'' will be counted as two different words. In Python, you can lower-case a word by calling ``lower()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Apples becomes apples\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\" == \"apples\")\n",
    "print(\"Apples becomes\", \"Apples\".lower())\n",
    "print(\"Apples\".lower() == \"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider words and not symbols or numbers. To test whether a word is a word, that is, consisting of only English characters, we can use ``isalpha()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\".isalpha())\n",
    "print(\"Apples123\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "def get_counts(data):\n",
    "    \"\"\" \n",
    "    counts the number of times a word appears in negative or positive tweets\n",
    "    \n",
    "    Parameters:\n",
    "    data: Pandas dataframe of tweets\n",
    "    \n",
    "    Returns:\n",
    "    counts: Dictionary of counts, which includes the dictionaries 'pos' and 'neg'\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    counts = {\"pos\":{}, \"neg\":{}}\n",
    "    posnegArray = [(data[0][i], data[5][i]) for i in range(0, len(data[0]))] \n",
    "    \n",
    "    count = 0\n",
    "    for sen,tweet in posnegArray:\n",
    "        tweetWords = word_tokenize(tweet)\n",
    "        if sen == 4:\n",
    "            checkInDictonary(tweetWords,counts,'pos')\n",
    "        elif sen == 0:\n",
    "            checkInDictonary(tweetWords,counts,'neg')\n",
    "    \n",
    "    # Your code goes here\n",
    "    return counts\n",
    "\n",
    "\n",
    "#This function will check if it is in the pos or negative dictonary already\n",
    "def checkInDictonary(tweetWords,counts,posOrneg):\n",
    "    tweetWords = filter(lambda x: x.isalpha(),tweetWords)\n",
    "    tweetWords = [x.lower() for x in tweetWords]\n",
    "    for word in tweetWords:\n",
    "        if word not in counts[posOrneg]:\n",
    "            counts[posOrneg][word] = 1\n",
    "        else:\n",
    "            counts[posOrneg][word] +=1\n",
    "    \n",
    "    \n",
    "# Do not change\n",
    "counts = get_counts(sentiment_data)\n",
    "print(counts[\"pos\"][\"happy\"]) # should print 124\n",
    "print(counts[\"neg\"][\"hate\"]) # should print 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculating $P(\\text{word}|\\text{polarity})$ (10 points)\n",
    "\n",
    "Create a function ``get_word_prob(counts, word, polarity)``, where ``counts`` is a dictionary like in the previous task, ``word`` is the word for which $P(word|polarity)$ will be calculated, and ``polarity`` is either ``pos`` or ``neg``. The function should return $P(word|polarity)$. If ``counts[polarity]`` does not contain ``word``, then return 0.\n",
    "\n",
    "Note that you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts, word, polarity``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002558903051371186\n",
      "0.00012162305973212521\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def get_word_prob(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    calculates the probability of a word given a polarity \n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    probability (float):  the probability of a word given a polarity \n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    posOrnegDict = counts[polarity]\n",
    "    wordSum = getTotalCounts(posOrnegDict)\n",
    "    \n",
    "    if word in posOrnegDict:\n",
    "        wordFre = posOrnegDict[word]\n",
    "        probability = wordFre/wordSum\n",
    "    else:\n",
    "        probability = 0\n",
    "        \n",
    "    return probability # P(word|polarity)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function: getTotalCounts\n",
    "Purpose: To add up the counts for each word\n",
    "\"\"\"\n",
    "def getTotalCounts(posOrnegDict):\n",
    "    wordsum = 0\n",
    "    for value in posOrnegDict.values():\n",
    "        wordsum += value\n",
    "    return wordsum\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Do not change\n",
    "print(get_word_prob(counts, \"great\", \"pos\")) # should print 0.00255902660421998\n",
    "print(get_word_prob(counts, \"glad\", \"neg\")) # should print 0.00012164155275442091\n",
    "print(get_word_prob(counts, \"wugs\", \"neg\")) # should print 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Calculate the log odds ratio of a word  (10 points)\n",
    "\n",
    "\n",
    "Using the above function, we can calculate $P(word|pos)$ and $P(word|neg)$ given a word, so we are ready to calculate the log odds ratio of that word as well. Create a function ``log_odds_ratio(count_dict, word, polarity)``, where the arguments are of the same type/format as in the previous problem. The function should return $log\\_odds\\_ratio(word)$:\n",
    "\n",
    "$$ log\\_odds\\_ratio(word, polarity) = \\log\\frac{P(word|polarity)}{P(word|opposite\\_polarity)} $$\n",
    "\n",
    "If the denominator is zero, return a very large number (eg 10000). Again you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts``, ``word``, and ``polarity``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.275701302546381\n",
      "-0.09165794206814183\n",
      "9999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def log_odds_ratio(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    This function returns the log odds ratio of a term (see previous cell)\n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    log_odds_ratio (float): log( prob(word|plarity) / P(word|opposity_polarity) )\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    wordProb = get_word_prob(counts, word, polarity)\n",
    "    wordProbOp = get_word_prob(counts, word, ('pos' if polarity == 'neg' else 'neg'))\n",
    "    \n",
    "    if wordProb ==0 or wordProbOp == 0:\n",
    "        return 9999999999\n",
    "    log_odds_ratio = math.log(wordProb/wordProbOp)\n",
    "    return log_odds_ratio\n",
    "\n",
    "\n",
    "# Do not change\n",
    "print(log_odds_ratio(counts, \"great\", \"pos\")) # should print 1.2755975445193852\n",
    "print(log_odds_ratio(counts, \"the\", \"neg\")) #  should print -0.09155418404114618\n",
    "print(log_odds_ratio(counts, \"wug\", \"neg\")) # should print a very large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Sorting log odds ratios (10 points)\n",
    "\n",
    "After being able to calculate log odds ratios for individual words, we can now sort words according to its association with a polarity class, say, positive. Create a function ``sort_pos_words(data)``, that takes in the entire dataframe as an argument, and return a sorted list of ``(word, log odds ratio)`` tuples for the positive sentiment class.\n",
    "\n",
    "If you implement this without filtering out any words, you will notice that there are many cases where the conditional probability of the denominator is 0, leading to the very large number you specified in the ``log_odds_ratio()`` function. This is because most words appear only once in the dataset. One way to mitigate this issue is to consider only words that appeared at least $x$ times in the dataset; here, let's only include words that appeared more than 10 times in the dataset, regardless of the polarity of the tweet (positive or negative).\n",
    "\n",
    "Use your function to print out the top 10 most positive:\n",
    "\n",
    "`` [('congratulations', 10000), ('proud', 10000), ('vip', 2.764994903138418), ('yum', 2.6218940594977447), ('mothers', 2.4548399748345786), ('worry', 2.4548399748345786), ('thank', 2.381508701749029), ('fabulous', 2.3595297950302534), ('sir', 2.3595297950302534), ('jonasbrothers', 2.3595297950302534)]``\n",
    "       \n",
    " and the top 10  most negative:\n",
    " \n",
    "`` [('expensive', -2.508004655425329), ('bus', -2.5821126275790505), ('throat', -2.651105499066002), ('hates', -2.651105499066002), ('tummy', -2.715644020203573), ('sad', -2.8052561788932606), ('missing', -2.987577735687215), ('died', -3.121109128311738), ('headache', -3.4694158225799536), ('hurts', -3.8348755960744185)]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most positive \n",
      " [('proud', 9999999999), ('congratulations', 9999999999), ('vip', 2.764994903138418), ('yum', 2.6218940594977447), ('worry', 2.4548399748345786), ('mothers', 2.4548399748345786), ('thank', 2.381508701749029), ('jonasbrothers', 2.3595297950302534), ('sir', 2.3595297950302534), ('fabulous', 2.3595297950302534)]\n",
      "Top 10 most negative \n",
      " [('expensive', -2.508004655425329), ('bus', -2.5821126275790505), ('hates', -2.651105499066002), ('throat', -2.651105499066002), ('tummy', -2.715644020203573), ('sad', -2.8052561788932606), ('missing', -2.987577735687215), ('died', -3.121109128311738), ('headache', -3.4694158225799536), ('hurts', -3.8348755960744185)]\n"
     ]
    }
   ],
   "source": [
    "def sort_pos_words(data):\n",
    "    \"\"\"\n",
    "    takes in a pandas dataframe and outputs the top 10 most positive and negative words in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): the tweets in a dataframe\n",
    "    \n",
    "    Return:\n",
    "    sorted_list (list): a sorted list of (word, log odds ratio) tuples for the positive sentiment class\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here \n",
    "    counts = get_counts(data)\n",
    "    \n",
    "    tupleList = []\n",
    "    for word,freq in counts['pos'].items():\n",
    "        wordCount = counts['pos'][word] if word not in counts['neg'] else counts['pos'][word] + counts['neg'][word]\n",
    "        if wordCount > 10:\n",
    "            tupleList.append((word, log_odds_ratio(counts,word,'pos')))\n",
    "    \n",
    "    sorted_list = sorted(tupleList, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return sorted_list\n",
    "\n",
    "    \n",
    "# Do not change\n",
    "lst = sort_pos_words(sentiment_data)\n",
    "print(\"Top 10 most positive \\n\", lst[:10]) # see previous cell for what this should print\n",
    "print(\"Top 10 most negative \\n\", lst[-10:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
